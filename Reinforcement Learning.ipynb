{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(\"\"\"\n",
    "<style>\n",
    "\n",
    "div.cell { /* Tunes the space between cells */\n",
    "margin-top:1em;\n",
    "margin-bottom:1em;\n",
    "}\n",
    "\n",
    "div.text_cell_render h1 { /* Main titles bigger, centered */\n",
    "font-size: 2.2em;\n",
    "line-height:1.4em;\n",
    "text-align:center;\n",
    "}\n",
    "\n",
    "div.text_cell_render h2 { /*  Parts names nearer from text */\n",
    "margin-bottom: -0.4em;\n",
    "}\n",
    "\n",
    "\n",
    "div.text_cell_render { /* Customize text cells */\n",
    "font-family: 'Times New Roman';\n",
    "font-size:1.5em;\n",
    "line-height:1.4em;\n",
    "padding-left:3em;\n",
    "padding-right:3em;\n",
    "}\n",
    "</style>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many machine learning tasks are in their core designing a model $\\rho$ to approximate some unknown function $f$. Given an element from the input space, we desire that\n",
    "$$ \\rho(x) \\approx f(x). $$\n",
    "\n",
    "In supervised learning tasks we have some observations $f(x_i)$ which we would like to generalize in order to predict the result for other inputs, this is usually done in a regression task where we attempt to minimize the mean square error by tuning the parameters of our model $\\Theta_\\alpha$.\n",
    "$$ \\min_{\\alpha} \\sum_i \\Big(\\big|\\rho(x_i) - f(x_i)\\big|^2\\Big) $$\n",
    "Unlike many other unsupervised learning tasks, the goal of reinforcement learning is also to estimate some unknown function. However, either a useful error function to optimize is unavailable or we don't have access to meaningful estimates of the unknown function $f$ and we are unable to perform a regression task. We do however have some form of signal that we can view that tells us whether or not our model is succeeding or failing.\n",
    "\n",
    "$e.g.$ While training a self-driving car, if we arrive safely at our destination, then we succeeded! If we crashed, however, we failed quite badly. The function we are trying to learn is the correct action to take given all sensory data involved while driving. We may not know what the absolute correct action is for all given scenarios, but we do have access to data for whether or not our current model is succeeding at the task.\n",
    "\n",
    "$e.g.$ Playing a game of Tic-Tac-Toe. We may not have data for what the correct response is to a given board, but we do however know when we win a game and when we lose a game."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rewards and Punishments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to set up a system of rewards and punishments for when certain events occur, $e.g.$ arriving safely at our destination, or winning a game of tic-tac-toe will give us a reward, while crashing or losing will give us a punishment. The function we would like to learn is which actions give us the highest future reward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-learning\n",
    "\n",
    "A Q-function $Q(s\\,|\\,a)$, also known as an action-value function, is a function which gives the maximum future reward given a state and an action. We can think of this function as what we would like to optimize. The Bellman equation is a recursive equation which gives us a means to estimate the Q-function:\n",
    "$$ Q(s\\, |\\, a) \\sim r + \\gamma\\max_{a'} Q(s'\\,|\\,a') $$\n",
    "where $r$ is the reward for taking action $a$, $\\gamma$ is a discount factor which we will explain shortly, $s'$ is the state that is induced by taking action $a$ on state $s$, and $\\{a'\\}$ are the set of possible actions for state $s'$. The discount factor $\\gamma <1$ is introduced so that we weight short-term rewards higher than long-term rewards. Usually, you solve the Bellman equation iteratively:\n",
    "$$ Q(s\\, |\\, a)  = (1-\\alpha) Q(s\\, |\\, a) + \\alpha( r + \\gamma\\max_{a'} Q(s'\\,|\\,a')) $$\n",
    "for some small $\\alpha$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal difference learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temporal difference learning $TD(0)$ is equivalent to learning a Q-function via solving iteratively applying the Bellman Equation. $TD(\\lambda)$ was first introduced by Gerry Tesauro in 1992 to play backgammon. His program: TD-gammon famously defeated the top human players at the time. $TD(\\lambda)$ attempts to estimate the Q-function for its entire history using its current estimate of the Q-function.\n",
    "$$ Q(s\\,|\\,a) \\sim (1-\\lambda) \\sum_{i=1}^{T-1} \\lambda^{i} \\Big(r^{(i-1)} + \\gamma\\max_{a^{(i)}} Q\\big(s^{(i)}\\,|\\,a^{(i)}\\big)\\Big) + \\lambda^T r^{(T)}$$\n",
    "where $s^{(i+1)}$ is the state that results from using action $a^{(i)}$ on state $s^{(i)}$ and $T$ denotes a terminal state. An interesting limit of $TD(\\lambda)$ is the case where $\\lambda \\to 1$. $TD(1)$ is also known as Monte-Carlo learning, and the Q-value update is given by\n",
    "$$Q(s\\,|\\,a) \\sim r^{(T)}, $$\n",
    "$i.e.$ only the estimated final reward is used for learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gridworld!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import random\n",
    "import IPython\n",
    "import sys\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (12, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grid = np.zeros((6,7))\n",
    "Q = random.random((6,7)) * 0.2 - 0.1\n",
    "treasure = 4,3\n",
    "traps = [(3,3),(4,2)]\n",
    "\n",
    "\n",
    "grid[0,:]= -np.inf\n",
    "grid[-1,:] = -np.inf\n",
    "grid[:,0] = -np.inf\n",
    "grid[:,-1] = -np.inf\n",
    "grid[treasure] = 1\n",
    "for trap in traps:\n",
    "    grid[trap] = -0.5\n",
    "\n",
    "Q[0,:]= -np.inf\n",
    "Q[-1,:] = -np.inf\n",
    "Q[:,0] = -np.inf\n",
    "Q[:,-1] = -np.inf\n",
    "\n",
    "\n",
    "print Q\n",
    "print grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "discount = 0.75\n",
    "def get_moves(grid,coord):\n",
    "    x,y=coord\n",
    "    neigh = [(x-1,y),(x+1,y),(x,y-1),(x,y+1)]\n",
    "    return [move for move in neigh if grid[move] != - np.inf]\n",
    "def move(Q,grid,coord,epsilon):\n",
    "    moves = get_moves(grid,coord)\n",
    "    if epsilon > random.random():\n",
    "        return moves[random.randint(0, len(moves))]\n",
    "    else:\n",
    "        val = -np.inf\n",
    "        for move in moves:\n",
    "            if discount * Q[move] + grid[move] > val:\n",
    "                val = discount * Q[move] + grid[move]\n",
    "                cand = move\n",
    "        return cand\n",
    "    \n",
    "def learn(Q,grid,coord,action,learning_rate):\n",
    "    Q[coord] = (1-learning_rate)*Q[coord] + learning_rate *(grid[action] + discount * Q[action])\n",
    "    return Q\n",
    "    \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start = (1,1)\n",
    "coord = (1,1)\n",
    "learning_rate = 0.5\n",
    "discount = 0.95\n",
    "epsilon = 0.1\n",
    "for session in range(1000):\n",
    "    action = move(Q,grid,coord,epsilon)\n",
    "    Q = learn(Q,grid,coord,action,learning_rate)\n",
    "    if action == treasure:\n",
    "        epsilon = 0.2 + (epsilon-0.2)*0.9\n",
    "        coord = treasure\n",
    "        plot_coord = coord[1]-1,coord[0]-1\n",
    "        plt.matshow(Q[1:-1,1:-1], vmin = -2,vmax =2,cmap = 'cool')\n",
    "        plt.scatter(*plot_coord, s = 2000, color = '#00EE00')\n",
    "        IPython.display.clear_output(wait=True)\n",
    "        IPython.display.display(plt.gcf())\n",
    "        plt.close()\n",
    "        coord = start\n",
    "    else:\n",
    "        coord = action\n",
    "    \n",
    "    plot_coord = coord[1]-1,coord[0]-1-0.25\n",
    "    plot_treasure = treasure[1]-1,treasure[0]-1+0.25\n",
    "    plot_traps = [(trap[1]-1,trap[0]-1+0.25) for trap in traps]\n",
    "    plt.matshow(Q[1:-1,1:-1], vmin = -1,vmax =1,cmap = 'cool_r')\n",
    "    \n",
    "    plt.scatter(*plot_treasure, s = 2000, color = 'yellow')\n",
    "    for plot_trap in plot_traps:\n",
    "        plt.scatter(*plot_trap, s = 2000, color = 'red')\n",
    "    plt.scatter(*plot_coord, s = 2000, color = '#00EE00')\n",
    "    plt.title(\"Move: {:d}, Exploration: {:f}\".format(session,epsilon),fontsize = 36)\n",
    "    IPython.display.clear_output(wait=True)\n",
    "    \n",
    "    \n",
    "    IPython.display.display(plt.gcf())\n",
    "    plt.close()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('grid_bot.pkl','wb') as file_:\n",
    "    pickle.dump(Q,file_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('grid_bot.pkl','rb') as file_:\n",
    "    Q = pickle.load(file_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
