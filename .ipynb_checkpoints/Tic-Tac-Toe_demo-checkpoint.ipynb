{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sknn\n",
    "from sknn import mlp\n",
    "import tic_tac_toe as ttt\n",
    "from numpy import random\n",
    "import IPython\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (12, 12)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('value_policy_3.pkl','rb') as file_:\n",
    "    value,policy = pickle.load(file_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "value = mlp.Regressor([mlp.Layer('Rectifier',units = 200),\n",
    "               mlp.Layer('Tanh',units = 1)],\n",
    "              learning_rate = 0.001,\n",
    "              batch_size = 1,\n",
    "              n_iter = 1)\n",
    "value.fit(np.zeros((16,9)) , np.zeros(16))\n",
    "\n",
    "policy = mlp.Regressor([mlp.Layer('Rectifier',units = 1000),\n",
    "                        #mlp.Layer('Rectifier',units = 250),\n",
    "               mlp.Layer('Tanh',units = 9,dropout=0.)],\n",
    "              learning_rate = 0.01,\n",
    "              batch_size = 10,\n",
    "              n_iter = 1)\n",
    "policy.fit(np.zeros((1,9)) , np.zeros((1,9)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Temp = 0.1\n",
    "class value_bot():\n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "    def make_move(self,board,turn):\n",
    "        return value_move(board,turn,self.value)\n",
    "def value_move(board,turn,value):\n",
    "    board = np.copy(board)\n",
    "    if turn == -1:\n",
    "        board = -1 * board\n",
    "    options = ttt.available_moves(board)\n",
    "    values = {}\n",
    "    for move in options:\n",
    "        ttt.update_move(board,move,1)\n",
    "        if ttt.is_over(board):\n",
    "            values[move] = ttt.winner(board)\n",
    "        else:\n",
    "            values[move] = -1 * value.predict(-1 * board.reshape((1,9)))[0]\n",
    "        ttt.unupdate_move(board,move)\n",
    "    action_values = values.items()\n",
    "    probs = softmax(np.array(zip(*action_values)[1]))\n",
    "    r = random.random()\n",
    "    for i in range(len(probs)):\n",
    "        r-=probs[i]\n",
    "        if r <0:\n",
    "            move = i\n",
    "            break\n",
    "    return action_values[move]\n",
    "        \n",
    "class policy_bot():\n",
    "    def __init__(self, policy):\n",
    "        self.policy = policy\n",
    "    def make_move(self,board,turn):\n",
    "        return policy_move(board,turn,self.policy)\n",
    "def policy_move(board,turn,policy):\n",
    "    board = np.copy(board)\n",
    "    if turn == -1:\n",
    "        board = -1 * board\n",
    "    action_values = zip(range(9),list(policy.predict(board.reshape(1,(9)))[0]))\n",
    "    #print action_values\n",
    "    probs = softmax(np.array(zip(*action_values)[1]))\n",
    "    #print probs\n",
    "    r = random.random()\n",
    "    for i in range(len(probs)):\n",
    "        r-=probs[i]\n",
    "        if r <0:\n",
    "            move = i\n",
    "            break\n",
    "    return action_values[move]\n",
    "\n",
    "def sym_policy(state,policy):\n",
    "    state = state.reshape((3,3))\n",
    "    states = [np.copy(state), np.copy(state.T)]\n",
    "    for i in range(3):\n",
    "        state = np.rot90(state)\n",
    "        states += [np.copy(state),np.copy(state.T)]\n",
    "    states = np.array(states)\n",
    "    pols = policy.predict(states).reshape(8,3,3)\n",
    "    pols[1] = pols[1].T\n",
    "    pols[2] = np.rot90(pols[2],3)\n",
    "    pols[3] = np.rot90(pols[3].T,3)\n",
    "    pols[4] = np.rot90(pols[4],2)\n",
    "    pols[5] = np.rot90(pols[5].T,2)\n",
    "    pols[6] = np.rot90(pols[6],1)\n",
    "    pols[7] = np.rot90(pols[7].T,1)\n",
    "    vals = sum(pols) * 0.125\n",
    "    return vals\n",
    "    \n",
    "def softmax(ar):\n",
    "#    print ar\n",
    "    ar -= np.max(ar)\n",
    "    exp = np.exp(ar / Temp)\n",
    "    return exp / np.sum(exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "replay = []\n",
    "history = []\n",
    "discount = 1\n",
    "X_train= []\n",
    "y_train=[]\n",
    "rewards = []\n",
    "actions = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "f, ((ax11, ax12 ,ax13,ax14),(ax21, ax22 ,ax23,ax24),(ax31, ax32 ,ax33,ax34)) = plt.subplots(3, 4, sharey=True)\n",
    "f.suptitle(\"Games Played: {:d}\".format(game),fontsize = 36)\n",
    "\n",
    "axes = []\n",
    "for i in range(1,13):\n",
    "    axes.append(plt.subplot(3,4,i, aspect='equal', adjustable='box-forced'))\n",
    "for game in range(1000000):\n",
    "    \n",
    "        \n",
    "    result = {'log' : [],\n",
    "              'winner' : None,\n",
    "              'boards' :[]}\n",
    "    board = ttt.empty_board()\n",
    "    turn = 1\n",
    "    t0 = time.clock()\n",
    "    while not ttt.is_over(board):\n",
    "        result['boards'].append(np.copy(board))\n",
    "        old_board = np.copy(board)\n",
    "        action,_ = policy_move(board,1,policy)\n",
    "        result['log'] += [action]\n",
    "        if board[action] == 0:\n",
    "            ttt.update_move(board,action,1)\n",
    "        else:\n",
    "            result['winner'] = -1*turn*2\n",
    "            break\n",
    "        if ttt.is_over(board):\n",
    "            result['winner'] = turn * ttt.winner(board)\n",
    "        board = -1 * board\n",
    "        turn =  -1 * turn\n",
    "    t1 = time.clock()\n",
    "    \n",
    "#     history.append(result)\n",
    "    train_index = random.randint(0, len(result['log']))\n",
    "    train_action = result['log'][train_index]\n",
    "    train_board = result['boards'][train_index]\n",
    "    reward = result['winner'] * (1 - (train_index%2) * 2)  #swap result if board is from player 2's perspective\n",
    "    \n",
    "    ############################################################\n",
    "    #################### train on the result ###################\n",
    "    ############################################################\n",
    "\n",
    "    X_train.append(train_board)\n",
    "    rewards.append(reward)\n",
    "    actions.append(train_action)\n",
    "    \n",
    "    if len(X_train) >=100:\n",
    "        y_train = map(lambda x: policy.predict(x.reshape((1,9)))[0] , X_train)\n",
    "        for i , tar in enumerate(y_train):\n",
    "            tar[actions[i]] = rewards[i]\n",
    "        \n",
    "        #replay += zip(X_train,y_train)\n",
    "        \n",
    "        policy.fit(np.array(X_train),np.array(y_train))\n",
    "        value.fit(np.array(X_train),np.array(rewards))\n",
    "        X_train = []\n",
    "        rewards = []\n",
    "        actions = []\n",
    "    t2 = time.clock()\n",
    "    #print 'game time = {}, train time = {}'.format(t1-t0,t2-t1)\n",
    "    \n",
    "    if game%1000 == 0:\n",
    "        f.suptitle(\"Games Played: {:d}\".format(game),fontsize = 36)\n",
    "        for ax in axes:\n",
    "            ax.cla()\n",
    "        \n",
    "        state = np.array([0,0,0,0,-1,0,0,0,0]).reshape((3,3))\n",
    "        answer = np.array([0,-1,0,-1,-1,-1,0,-1,0]).reshape((3,3))\n",
    "        opinion = policy.predict(state.reshape((1,9)))[0].reshape((3,3))\n",
    "\n",
    "        sym = sym_policy(state,policy)\n",
    "                \n",
    "        \n",
    "        #plt.figure(1)\n",
    "\n",
    "        axes[0].matshow(state,vmin=-1,vmax=1,cmap = 'seismic')\n",
    "        #ax1.set_aspect('equal')\n",
    "        #plt.figure(2)\n",
    "        \n",
    "        axes[1].matshow(opinion,vmin=-1,vmax=1,cmap = 'seismic_r')\n",
    "        \n",
    "        \n",
    "        axes[2].matshow(sym,vmin=-1,vmax=1,cmap = 'seismic_r')\n",
    "        #ax1.set_aspect('equal')\n",
    "        \n",
    "        axes[3].matshow(answer,vmin=-1,vmax=1,cmap = 'seismic_r')\n",
    "        state = np.array([-1,0,-1,0,0,0,1,1,0]).reshape((3,3))\n",
    "        answer = np.array([-1,discount**2,-1,-discount,-discount,-discount,-1,-1,1]).reshape((3,3))\n",
    "        opinion = policy.predict(state.reshape((1,9)))[0].reshape((3,3))\n",
    "        sym = sym_policy(state,policy)\n",
    "        \n",
    "        #plt.figure(1)\n",
    "        axes[4].matshow(state,vmin=-1,vmax=1,cmap = 'seismic')\n",
    "        #ax1.set_aspect('equal')\n",
    "        #plt.figure(2)\n",
    "        axes[5].matshow(opinion,vmin=-1,vmax=1,cmap = 'seismic_r')\n",
    "        \n",
    "        axes[6].matshow(sym,vmin=-1,vmax=1,cmap = 'seismic_r')\n",
    "        #ax1.set_aspect('equal')\n",
    "        axes[7].matshow(answer,vmin=-1,vmax=1,cmap = 'seismic_r')\n",
    "        state = np.array([-1,0,-1,-1,1,1,1,1,-1]).reshape((3,3))\n",
    "        answer = np.array([-1,1,-1,-1,-1,-1,-1,-1,-1]).reshape((3,3))\n",
    "        opinion = policy.predict(state.reshape((1,9)))[0].reshape((3,3))\n",
    "        sym = sym_policy(state,policy)\n",
    "        #plt.figure(1)\n",
    "        axes[8].matshow(state,vmin=-1,vmax=1,cmap = 'seismic')\n",
    "        #ax1.set_aspect('equal')\n",
    "        #plt.figure(2)\n",
    "        axes[9].matshow(opinion,vmin=-1,vmax=1,cmap = 'seismic_r')\n",
    "        \n",
    "        axes[10].matshow(sym,vmin=-1,vmax=1,cmap = 'seismic_r')\n",
    "        #ax1.set_aspect('equal')\n",
    "        axes[11].matshow(answer,vmin=-1,vmax=1,cmap = 'seismic_r')\n",
    "        IPython.display.clear_output(wait=True)\n",
    "        IPython.display.display(plt.gcf())\n",
    "        \n",
    "        with open('value_policy3.pkl','wb') as file_:\n",
    "            pickle.dump((value,policy),file_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('value_policy3.pkl','rb') as file_:\n",
    "    value,policy = pickle.load(file_)\n",
    "    \n",
    "f,_ = plt.subplots(3, 4, sharey=True)   \n",
    "axes = []\n",
    "for i in range(1,13):\n",
    "    axes.append(plt.subplot(3,4,i, aspect='equal', adjustable='box-forced'))\n",
    "f.suptitle(\"Games Played: {:d}\".format(game),fontsize = 36)\n",
    "for ax in axes:\n",
    "    ax.cla()\n",
    "\n",
    "state = np.array([0,0,0,0,-1,0,0,0,0]).reshape((3,3))\n",
    "answer = np.array([0,-1,0,-1,-1,-1,0,-1,0]).reshape((3,3))\n",
    "opinion = policy.predict(state.reshape((1,9)))[0].reshape((3,3))\n",
    "\n",
    "sym = sym_policy(state,policy)\n",
    "\n",
    "\n",
    "#plt.figure(1)\n",
    "\n",
    "axes[0].matshow(state,vmin=-1,vmax=1,cmap = 'seismic')\n",
    "#ax1.set_aspect('equal')\n",
    "#plt.figure(2)\n",
    "\n",
    "axes[1].matshow(opinion,vmin=-1,vmax=1,cmap = 'seismic_r')\n",
    "\n",
    "\n",
    "axes[2].matshow(sym,vmin=-1,vmax=1,cmap = 'seismic_r')\n",
    "#ax1.set_aspect('equal')\n",
    "\n",
    "axes[3].matshow(answer,vmin=-1,vmax=1,cmap = 'seismic_r')\n",
    "state = np.array([-1,0,-1,0,0,0,1,1,0]).reshape((3,3))\n",
    "answer = np.array([-1,discount**2,-1,-discount,-discount,-discount,-1,-1,1]).reshape((3,3))\n",
    "opinion = policy.predict(state.reshape((1,9)))[0].reshape((3,3))\n",
    "sym = sym_policy(state,policy)\n",
    "\n",
    "#plt.figure(1)\n",
    "axes[4].matshow(state,vmin=-1,vmax=1,cmap = 'seismic')\n",
    "#ax1.set_aspect('equal')\n",
    "#plt.figure(2)\n",
    "axes[5].matshow(opinion,vmin=-1,vmax=1,cmap = 'seismic_r')\n",
    "\n",
    "axes[6].matshow(sym,vmin=-1,vmax=1,cmap = 'seismic_r')\n",
    "#ax1.set_aspect('equal')\n",
    "axes[7].matshow(answer,vmin=-1,vmax=1,cmap = 'seismic_r')\n",
    "state = np.array([-1,0,-1,-1,1,1,1,1,-1]).reshape((3,3))\n",
    "answer = np.array([-1,1,-1,-1,-1,-1,-1,-1,-1]).reshape((3,3))\n",
    "opinion = policy.predict(state.reshape((1,9)))[0].reshape((3,3))\n",
    "sym = sym_policy(state,policy)\n",
    "#plt.figure(1)\n",
    "axes[8].matshow(state,vmin=-1,vmax=1,cmap = 'seismic')\n",
    "#ax1.set_aspect('equal')\n",
    "#plt.figure(2)\n",
    "axes[9].matshow(opinion,vmin=-1,vmax=1,cmap = 'seismic_r')\n",
    "\n",
    "axes[10].matshow(sym,vmin=-1,vmax=1,cmap = 'seismic_r')\n",
    "#ax1.set_aspect('equal')\n",
    "axes[11].matshow(answer,vmin=-1,vmax=1,cmap = 'seismic_r')\n",
    "IPython.display.clear_output(wait=True)\n",
    "IPython.display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "# a talk about IPython at Sage Days at U. Washington, Seattle.\n",
    "# Video credit: William Stein.\n",
    "YouTubeVideo('JNrXgpSEEIE',start = 28,width = 800,height = 600,)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
